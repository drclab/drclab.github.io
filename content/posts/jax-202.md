+++
title = "JAX 202: Bayesian Sampling with BlackJax"
date = "2025-10-31T00:00:00Z"
type = "post"
draft = true
math = true
tags = ["jax", "blackjax", "bayesian-inference"]
categories = ["posts"]
description = "Run Hamiltonian Monte Carlo and adaptive samplers in JAX with BlackJax, leveraging composable kernels and jit-friendly sampling loops."
+++

Welcome to JAX 202! After exploring vectorization and multi-device training, it is time to lean into probabilistic inference. BlackJax builds on core JAX primitives to provide Hamiltonian Monte Carlo (HMC), No-U-Turn Sampler (NUTS), and adaptive algorithms that jit into tight sampling loops. This guide shows how to go from a log-density function to converged posterior draws without leaving the JAX ecosystem.

You will learn how to (1) define targets as pure JAX functions, (2) set up BlackJax kernels and initial states, (3) run tuned warmup with `numpyro`-style interfaces, (4) vectorize chains for throughput, and (5) monitor diagnostics such as effective sample size (ESS) and the Gelman-Rubin statistic.

## 0. Environment setup

Install the probabilistic stack inside the same Colab or workstation environment used for previous lessons. You need BlackJax plus supporting libraries for transformations and diagnostics.

```python
%pip install -q -U "jax[cpu]" blackjax arviz optax
```

If GPUs are available, swap `"jax[cpu]"` for `"jax[cuda12_pip]"` to reuse accelerator support. BlackJax compiles kernels with `jit`, so ensure your environment tracks JAX ≥ 0.4.30 to avoid deprecation warnings.

## 1. Framing the posterior

BlackJax expects a callable returning the log-probability of the target distribution. Keep the function side-effect free and operate purely on JAX arrays.

```python
import jax
import jax.numpy as jnp

def logistic_regression_logprob(theta, x, y):
    weight, bias = theta[:-1], theta[-1]
    logits = jnp.dot(x, weight) + bias
    log_likelihood = jnp.sum(y * logits - jnp.logaddexp(0.0, logits))
    log_prior = -0.5 * jnp.sum(theta**2)  # standard normal prior
    return log_likelihood + log_prior
```

For binary labels $y_i \in \{0, 1\}$ with feature vectors $x_i \in \mathbb{R}^d$, the logistic regression log-likelihood is

$$
\log p(\mathbf{y} \mid \theta, X) = \sum_{i=1}^{N}\left[y_i \cdot (x_i^\top w + b) - \log\left(1 + e^{x_i^\top w + b}\right)\right],
$$

where $\theta = (w, b)$. The standard normal prior on the parameters contributes

$$
\log p(\theta) = -\tfrac{1}{2}\,\theta^\top \theta - \tfrac{d_\theta}{2} \log(2\pi),
$$

with $d_\theta = \dim(\theta) = d + 1$. In code we drop the constant $-\tfrac{d_\theta}{2} \log(2\pi)$ because BlackJax only needs the log-density up to an additive constant.

Combining both terms yields the log-posterior $\log p(\theta \mid X, \mathbf{y}) = \log p(\mathbf{y} \mid \theta, X) + \log p(\theta)$ that feeds into BlackJax.

Batch your data as host arrays or `ShapedArray` placeholders. The signature should be `(params, *data)` so that you can partially apply data and differentiate with respect to parameters.

```python
from functools import partial

target_logprob = partial(logistic_regression_logprob, x=train_features, y=train_labels)
```

Because BlackJax pulls gradients via `jax.grad`, any nondifferentiable operations inside `target_logprob` will surface immediately as `FilteredStackTrace` errors—fix them before continuing.

## 2. Initializing kernels and states

BlackJax HMC kernels are functional pairs of `(init, step)`. Initialize positions and momentum before entering the sampling loop.

```python
import blackjax

step_size = 1e-2
num_integration_steps = 10

init_state = blackjax.hmc.init(position=jnp.zeros(train_features.shape[1] + 1))
kernel = blackjax.hmc.build_kernel(target_logprob, step_size, num_integration_steps)
```

`init_state` stores the current position, momentum, and potential energy. To carry auxiliary data (e.g., running acceptance stats), unpack and re-pack the state each iteration—BlackJax structures are PyTrees, so they cooperate with `jit` and `pmap`.

## 3. Running adaptation and warmup

Rather than hand-tuning step sizes, call the adaptive warmup helper which mirrors Stan and NumPyro defaults. It returns tuned step sizes and mass matrices you can feed back into the kernel.

```python
num_warmup = 1000
rng_key = jax.random.key(0)
adapt = blackjax.window_adaptation.blackjax_default(num_warmup, target_logprob)

tuned_state, tuned_parameters, warmup_info = adapt.run(rng_key, init_state.position)
```

`run` executes a full warmup schedule (fast/slow windows, dual averaging) under `jit`. Inspect `warmup_info` for step-size trajectories or divergence counts if adaptation stalls.

Rebuild the kernel with the tuned parameters before drawing posterior samples.

```python
kernel = blackjax.hmc.build_kernel(
    target_logprob,
    tuned_parameters.step_size,
    tuned_parameters.inverse_mass_matrix,
)
state = blackjax.hmc.init(tuned_state.position, tuned_parameters.inverse_mass_matrix)
```

## 4. Sampling in a compiled loop

Use `jax.lax.fori_loop` or `jax.lax.scan` to iterate the kernel. Keep randomness threaded through the loop so each step sees an independent momentum refresh.

```python
def sample_hmc(rng_key, state, num_samples):
    def one_step(carry, _):
        rng_key, state = carry
        rng_key, subkey = jax.random.split(rng_key)
        state, info = kernel(subkey, state)
        return (rng_key, state), (state.position, info)

    (_, final_state), (positions, infos) = jax.lax.scan(
        one_step, (rng_key, state), xs=None, length=num_samples
    )
    return positions, infos, final_state

positions, infos, state = sample_hmc(rng_key, state, num_samples=2000)
```

Because everything is pure JAX, the first call traces the full sampling loop, and subsequent calls with the same shapes run at compiled speed. Avoid Python-side lists or conditional logging inside `one_step`; use `jax.debug.print` sparingly if divergence counts spike.

## 5. Vectorizing multiple chains

To launch several chains, vmap the kernel over independent seeds and initial positions. BlackJax kernels are reentrant, making `vmap` or `pmap` straightforward.

```python
chain_ids = jnp.arange(4)
chain_keys = jax.random.split(rng_key, len(chain_ids))
chain_positions = tuned_state.position + 0.01 * jax.random.normal(chain_keys, tuned_state.position.shape)

@jax.vmap
def run_chain(rng_key, initial_position):
    state = blackjax.hmc.init(initial_position, tuned_parameters.inverse_mass_matrix)
    samples, infos, _ = sample_hmc(rng_key, state, num_samples=1000)
    return samples, infos

samples, infos = run_chain(chain_keys, chain_positions)
```

With `pmap`, place the outer axis on devices to split chains across GPUs. Keep per-chain sample counts identical so the compiled graph remains static.

## 6. Monitoring diagnostics

Collect kernel metadata (acceptance rates, energy errors) from `infos` and feed samples to ArviZ for summary statistics.

```python
import arviz as az

idata = az.from_dict(
    posterior={"theta": samples},
    sample_stats={
        "accept_prob": infos.acceptance_probability,
        "energy": infos.energy,
    },
)

summary = az.summary(idata, var_names=["theta"], kind="stats")
print(summary[["mean", "sd", "ess_bulk", "r_hat"]])
```

Watch for:

- **Low ESS**: Increase trajectory length (`num_integration_steps`) or re-run warmup with a denser mass matrix.
- **`r_hat` > 1.01**: Seed more chains or extend the sampling run until Gelman-Rubin diagnostics stabilize.
- **Divergences**: Reduce step size and warm up again; divergences indicate energy discrepancies from the integrator.

## 7. Beyond basic kernels

- Try `blackjax.nuts` for path-length adaptation that removes the need to choose `num_integration_steps`.
- Swap in `blackjax.rmh` (randomized midpoint integrator) when you need higher-order accuracy on challenging posteriors.
- Combine BlackJax with [TFP JAX distributions](https://www.tensorflow.org/probability) or custom bijectors to reparameterize constrained variables.
- For online inference, roll your own `jax.lax.scan` that interleaves streaming data updates with kernel steps.

With the BlackJax toolbox, you can express rich probabilistic models while staying inside the jit/pmap-friendly subset of JAX. Practice on small datasets, watch diagnostics, and scale out once you trust your sampler.
