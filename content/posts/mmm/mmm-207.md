+++
title = "MMM 207: Running Examples as Method Guides"
date = 2026-02-14
type = "post"
draft = false
categories = ["posts", "stats"]
tags = ["marketing", "causal-inference", "mmm", "panel-data"]
description = "Three motivating panel vignettes and how they map to estimands, assumptions, and methods: loyalty rollouts, TV advertising carryover, and platform entry."
math = true
+++

## Why the running examples matter
Section 2.7 returns to three recurring vignettes and reframes them with the language of estimands and identification. The point is not to pick one favorite estimator. It is to show how **data shape**, **assignment**, and **causal target** jointly determine the right method and diagnostics.

The three examples span the major panel regimes:
- **Thin panels** with many units, few periods.
- **Fat panels** with few units, many periods.
- **Square panels** with similar $N$ and $T$.

Each one highlights a different combination of treatment type, assumptions, and risks.

## Example 1: Loyalty program with staggered rollout
**Setting.** A retailer observes 500 stores over 12 quarters and rolls out a loyalty program in waves. Treatment is binary and staggered: some stores adopt early, some late, some never.

**Estimands.** The primary target is the panel ATT over treated store-quarters. Cohort-time effects $\tau(g,t)$ and event-time effects $\theta_k$ are central for understanding whether effects grow with tenure.

**Identification.** This is a classic staggered-adoption design, so the key assumption is **parallel trends** across cohorts before adoption. Because loyalty programs can create spillovers (word-of-mouth or cross-shopping), SUTVA is also at risk, and exposure mappings may be required.

**Methods and diagnostics.**
- Modern DiD for $\tau(g,t)$ and ATT aggregation.
- Event studies for dynamics and pre-trend checks.
- Spillover models when geographic interference matters.
- Sensitivity analysis and placebo tests for robustness.

**Design lesson.** The right estimand is clear (ATT and dynamics), but credibility hinges on pre-trend evidence and the plausibility of limited spillovers.

## Example 2: TV advertising carryover across markets
**Setting.** A CPG brand tracks 50 DMAs over 100 weeks. Treatment is continuous (GRPs). The panel is fat: long time series, few markets.

**Estimands.** The target is a **dose-response function** $\mu(d)=\mathbb{E}[Y_{it}(d)]$ and its derivative (marginal effect). Dynamic estimands such as the long-run multiplier and half-life summarize carryover from a distributed-lag profile $\{\gamma_s\}$.

**Identification.** Endogeneity is the core risk: spend responds to anticipated demand. Parallel trends is less natural for continuous treatment; instead, identification relies on **conditional unconfoundedness** with rich controls or a **factor structure** that isolates common demand shocks.

**Methods and diagnostics.**
- Distributed lag models for dynamics and carryover.
- DML or high-dimensional controls to adjust for many confounders.
- Factor models to absorb common shocks with heterogeneous loadings.
- HAC or two-way clustered inference for long, serially correlated panels.

**Design lesson.** In continuous-treatment settings, the estimator is only as good as the control strategy. Diagnostics should focus on sensitivity to controls and stability of dynamic responses.

## Example 3: Platform entry and competitive dynamics
**Setting.** A delivery platform enters 30 cities over 24 months (binary, staggered). Outcomes are restaurant revenues in 50 cities. The panel is roughly square.

**Estimands.** ATT over treated city-months is the main target, with $\tau(g,t)$ and $\theta_k$ to separate short-run disruptions from long-run adjustments. Spillover-aware estimands are also relevant because competitors respond to entry.

**Identification.** Parallel trends may be plausible only after covariate adjustment, and a **factor structure** can help when common shocks affect cities differently. Spillovers from competitive responses violate SUTVA and require explicit modeling or bounds.

**Methods and diagnostics.**
- Synthetic control for each treated city when comparables exist.
- SDID and factor models to combine pre-trend fit with panel structure.
- Spillover models to separate direct and competitive effects.
- Placebo tests and leave-one-out checks for robustness.

**Design lesson.** When selection is strong and comparables are scarce, identification should lean on pre-treatment fit and factor structure, not naive parallel trends.

## Cross-example decision rules
Across all three vignettes, the same questions recur:
- **Which assumption justifies the comparison?** Parallel trends, unconfoundedness, or factor structure.
- **Which estimand answers the business question?** ATT for ROI, $\theta_k$ for dynamics, or dose-response for intensity.
- **Which diagnostics are most informative?** Pre-trends, placebo tests, and sensitivity analysis.

## Takeaway
The vignettes show how method selection flows from the data and the causal question. The same estimator can be appropriate in one example and misleading in another. The safest workflow is: define the estimand, articulate the assignment mechanism, match the identification assumption, then choose the estimator and diagnostics that make those assumptions visible.

## References
- Shaw, C. (2025). *Causal Inference in Marketing: Panel Data and Machine Learning Methods* (Community Review Edition), Section 2.7.
- Callaway, B., and Sant'Anna, P. H. C. (2021). Difference-in-differences with multiple time periods. *Journal of Econometrics*.
- Sun, L., and Abraham, S. (2021). Estimating dynamic treatment effects in event studies. *Journal of Econometrics*.
- Abadie, A., Diamond, A., and Hainmueller, J. (2010). Synthetic control methods for comparative case studies. *Journal of the American Statistical Association*.
