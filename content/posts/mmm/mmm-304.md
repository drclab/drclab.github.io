+++
title = "MMM 304: Switchbacks and Platform Experiments"
date = 2026-02-27
type = "post"
draft = false
categories = ["posts", "stats"]
tags = ["marketing", "causal-inference", "mmm", "panel-data"]
description = "Switchback designs for platform experiments: identification, carryover, washouts, learning systems, and pacing pitfalls."
math = true
+++

## Why switchbacks exist
Switchbacks randomize treatment **over time within the same unit** rather than across units. A city, market, or platform segment alternates between treatment and control according to a pre-specified schedule, so each unit serves as its own control. This is common when:
- Permanent treatment assignment is infeasible or unethical.
- Platforms prioritize rapid iteration and short-run learning.
- Cross-sectional randomization is limited by network or spillover risks.

In the design taxonomy, switchbacks hold the unit fixed and randomize over time blocks, contrasting with geo-experiments (randomize across units) and phased rollouts (staggered across cohorts).

## The basic switchback estimand
With no carryover and a well-defined blocking scheme, the estimand is the average within-unit effect across treated blocks:

$$
\tau = \mathbb{E}[Y_{it}(1)-Y_{it}(0)] \text{ over the randomized blocks.}
$$

Identification comes from the assignment mechanism: randomization across time blocks makes treated and control blocks comparable **within the same unit**.

## Blocking by time to control confounding
Time-varying confounders (day-of-week, hour-of-day, seasonal cycles) can bias simple alternation. The standard fix is **period blocking**:

- Block by day-of-week or hour-of-day.
- Randomize treatment within each block.
- Avoid over-blocking: too many small blocks reduce power.

Blocking mirrors stratification in geo-experiments. It is essential because switchbacks assume time blocks are comparable after conditioning on the block structure.

## The core threat: carryover
Switchbacks are fragile when effects persist. If treatment in period $t$ affects outcomes in $t+1$, then a control block is contaminated by lagged treatment. This biases the estimate toward zero.

A no-carryover restriction is often implicit:

$$
Y_{it}(d^{t}_{i}) = Y_{it}(d_{it}).
$$

This is strong. A more realistic option is finite memory (order-$L$ carryover):

$$
Y_{it}(d_{ti}) \approx Y_{it}(d_{it}, d_{i,t-1}, \ldots, d_{i,t-L}).
$$

The key design question becomes: how large is $L$ in your setting?

## Anticipation effects
Anticipation is the mirror image of carryover: if users or systems **expect** future treatment, they may change behavior in advance. Anticipation contaminates pre-treatment control blocks and undermines the switchback contrast. The result is the same: biased estimates that are hard to interpret without explicit modeling.

## Washout periods as a design lever
Washout periods insert gaps between treatment and control blocks, letting effects decay before the next control period. The washout length should be informed by the expected half-life of effects:

- If effects decay exponentially with half-life $h$, then two half-lives reduce effects to $0.25$ of their initial magnitude.
- If effects have delayed peaks or non-monotone decay, simple washout rules can fail.

Practical rule: treat washout choice as a sensitivity parameter. Vary it and check stability of estimates.

## When carryover is unavoidable
If carryover is strong and washouts are impractical, you must **model dynamics**:
- Distributed lag models include current and lagged treatments.
- Event-study specifications trace dynamic responses.

These approaches shift the estimand from an immediate effect to a dynamic profile, including cumulative impacts.

## Learning systems complicate identification
Platform experiments often run alongside adaptive systems (bidding, targeting, personalization). If the learning system reacts differently under treatment and control, the estimated effect conflates the intervention with optimizer dynamics.

Design-based options:
- **Freeze** the learning system during the experiment.
- **Isolate** the experiment in a separate environment.
- **Model** learning dynamics explicitly.

## External validity caveat
Freezing the learning system changes the estimand. You are no longer estimating the effect under production dynamics, but the effect **with the freeze in place**. If the learning system amplifies or dampens the intervention in production, the switchback estimate may not generalize.

## Pacing and budget exhaustion
Pacing controls how budgets or inventory are spent over time. If budgets run out early, later treated blocks receive no exposure, diluting the effect.

Two responses:
- **Design fix:** ensure budget and inventory cover the full window.
- **Modeling fix:** treat budget exhaustion as informative heterogeneity.

Caution: budget exhaustion is a post-treatment variable. Conditioning on it changes the estimand and can induce selection bias.

## When switchbacks are the right tool
Switchbacks are strongest when:
- Effects are short-lived or well-bounded in time.
- Time blocks can be balanced and randomized.
- Carryover is limited or explicitly modeled.
- Learning systems are stable or properly isolated.

When long-run or steady-state effects are the target, slower designs (geo-experiments or phased rollouts) are often more credible.

## Design checklist
- Define the block structure (day-of-week, hour-of-day).
- Justify a carryover horizon or plan for washouts.
- Verify anticipation is unlikely.
- Specify whether learning systems are frozen or active.
- Pre-commit pacing rules and budget sufficiency.
- Decide whether the estimand is immediate, cumulative, or steady-state.

## Takeaway
Switchbacks deliver fast learning but demand strong control over dynamics. The design must align with carryover behavior, learning systems, and pacing constraints. When those conditions are satisfied, switchbacks are powerful; when they fail, the estimates can be misleading even with perfect randomization.

## References
- Shaw, C. (2025). *Causal Inference in Marketing: Panel Data and Machine Learning Methods* (Community Review Edition), Section 3.4.
- Bojinov, I., et al. (2022). Time-series experimentation and switchback designs.
- Athey, S., et al. (2025a). Methods for dynamic treatment effects and platform experiments.
