{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltUy0s2PQIsY",
        "outputId": "e49a2474-85b8-468b-c792-868a01045104"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpyro\n",
            "  Downloading numpyro-0.19.0-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: jax>=0.4.25 in /usr/local/lib/python3.12/dist-packages (from numpyro) (0.7.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.25 in /usr/local/lib/python3.12/dist-packages (from numpyro) (0.7.2)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from numpyro) (1.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from numpyro) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from numpyro) (4.67.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.25->numpyro) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.25->numpyro) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax>=0.4.25->numpyro) (1.16.2)\n",
            "Downloading numpyro-0.19.0-py3-none-any.whl (370 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/370.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m370.9/370.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpyro\n",
            "Successfully installed numpyro-0.19.0\n"
          ]
        }
      ],
      "source": [
        "!pip install numpyro"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-TrnLA-rpp1",
        "outputId": "743720e7-8596-4c7d-be71-7780c1359083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JAX version: 0.7.2\n",
            "Backend: gpu\n",
            "Devices: [CudaDevice(id=0)]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "\n",
        "print(\"JAX version:\", jax.__version__)\n",
        "print(\"Backend:\", jax.default_backend())\n",
        "print(\"Devices:\", jax.devices())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYgy4A1sEIo",
        "outputId": "cab502ea-3ceb-4719-c46d-8ad2c75170a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Oct 29 18:34:31 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0             26W /   70W |     110MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACe8H-X9sL1R",
        "outputId": "b1afb0a6-a3cd-44c9-a559-36d2eaf13a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset shape: (569, 30) (569,)\n"
          ]
        }
      ],
      "source": [
        "import jax.numpy as jnp\n",
        "import numpyro\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "tfd = numpyro.distributions\n",
        "\n",
        "dataset = load_breast_cancer()\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(dataset.data).astype(\"float32\")\n",
        "y = dataset.target.astype(\"float32\")\n",
        "\n",
        "X = jnp.asarray(X)\n",
        "y = jnp.asarray(y)\n",
        "n_features = X.shape[1]\n",
        "print(\"Dataset shape:\", X.shape, y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ESClGIhSw1Q2"
      },
      "outputs": [],
      "source": [
        "def joint_log_prob(x, y, tau, lamb, beta):\n",
        "    lp = tfd.Gamma(0.5, 0.5).log_prob(tau)\n",
        "    lp += tfd.Gamma(0.5, 0.5).log_prob(lamb).sum()\n",
        "    lp += tfd.Normal(0.0, 1.0).log_prob(beta).sum()\n",
        "    logits = x @ (tau * lamb * beta)\n",
        "    lp += tfd.Bernoulli(logits=logits).log_prob(y).sum()\n",
        "    return lp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9szyGxAM09wR",
        "outputId": "433680d1-a6ab-48d9-cf14-0bd576d1cd92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.947667   0.9785799  0.33229148 0.46866846 0.5698887  0.16550303\n",
            " 0.3101946  0.68948054 0.74676657 0.17101455 0.9853538  0.02528262\n",
            " 0.6400418  0.56269085 0.8992138  0.93453753 0.8341402  0.7256162\n",
            " 0.5098531  0.02765214 0.03148878 0.9580188  0.5188192  0.79221416\n",
            " 0.5522419  0.6113529  0.8931755  0.75499094 0.21164179 0.22934973]\n"
          ]
        }
      ],
      "source": [
        "from jax import random\n",
        "\n",
        "key = random.key(0)           # initialize PRNG key\n",
        "beta = random.uniform(key, (30,), minval=0.0, maxval=1.0)\n",
        "\n",
        "print(beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGlmGLN013xG",
        "outputId": "5d667dbb-6c59-46e1-dc59-3120373b50b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array(-4869.8623, dtype=float32)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "joint_log_prob(X, y, 1.0, 1.0, beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GdsDqXRv26un"
      },
      "outputs": [],
      "source": [
        "def unconstrained_joint_log_prob(x, y, z):\n",
        "    ndims = x.shape[-1]\n",
        "    unc_tau, unc_lamb, beta = jnp.split(z, [1, 1 + ndims])\n",
        "    unc_tau = unc_tau.reshape([])\n",
        "    tau = jnp.exp(unc_tau)\n",
        "    lamb = jnp.exp(unc_lamb)\n",
        "    ldj = unc_tau + unc_lamb.sum()\n",
        "    return joint_log_prob(x, y, tau, lamb, beta) + ldj\n",
        "\n",
        "target_log_prob = lambda z: unconstrained_joint_log_prob(X, y, z)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNRKj1e23cJP",
        "outputId": "26334969-3b30-4235-fd6a-940ef824b92d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial log-density: -465.95599365234375\n",
            "Gradient L2 norm: 803.6372680664062\n"
          ]
        }
      ],
      "source": [
        "target_log_prob_and_grad = jax.value_and_grad(target_log_prob)\n",
        "\n",
        "dim = 1 + n_features + n_features  # tau + lamb + beta\n",
        "z_init = jnp.zeros((dim,))\n",
        "\n",
        "logp, grad = target_log_prob_and_grad(z_init)\n",
        "print(\"Initial log-density:\", float(logp))\n",
        "print(\"Gradient L2 norm:\", float(jnp.linalg.norm(grad)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8khUUUV53rKw"
      },
      "outputs": [],
      "source": [
        "from functools import partial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EHIH0iCYQ4FC"
      },
      "outputs": [],
      "source": [
        "target_log_prob = partial(unconstrained_joint_log_prob, X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Y9c9REsbRKyD"
      },
      "outputs": [],
      "source": [
        "target_log_prob_and_grad = jax.value_and_grad(target_log_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "la9PxSigReCo",
        "outputId": "b58cdd09-2de7-463e-e8ba-4882c2819550"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-465.956 \n",
            " [   0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.           0.           0.           0.           0.\n",
            "    0.        -200.83615   -114.22049   -204.30441   -195.0466\n",
            "  -98.642456  -164.11075   -191.57361   -213.6521     -90.92255\n",
            "    3.5317173 -156.02263      2.2843075 -152.99834   -150.82368\n",
            "   18.436588   -80.60622    -69.8029    -112.2554       1.7941825\n",
            "  -21.450775  -213.60806   -125.69727   -215.38536   -201.88058\n",
            " -115.948044  -162.58789   -181.46356   -218.31577   -114.52559\n",
            "  -89.099594 ]\n"
          ]
        }
      ],
      "source": [
        "tlp, tlp_grad = target_log_prob_and_grad(z_init)\n",
        "print(tlp,\"\\n\" ,tlp_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OuxQMlP9Tf4h"
      },
      "source": [
        "## 3.2 Hamiltonian Monte Carlo\n",
        "\n",
        "Now we are ready to move on to using HMC. Below, we implement a simple version of HMC using JAX. In particular, this implementation does not accept or adapt a mass matrix (Betancourt, 2018; Neal, 2011) (it is implicitly an identity matrix of the appropriate size), nor does it adapt the step size or number of leapfrog steps. We use the variable `z` for the parameters, and `m` for the momentum."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "gh0Oh3RRRpy2"
      },
      "outputs": [],
      "source": [
        "def leapfrog_step(target_log_prob_and_grad, step_size, i, leapfrog_state):\n",
        "    z, m, tlp, tlp_grad = leapfrog_state\n",
        "    m += 0.5 * step_size * tlp_grad\n",
        "    z += step_size * m\n",
        "    tlp, tlp_grad = target_log_prob_and_grad(z)\n",
        "    m += 0.5 * step_size * tlp_grad\n",
        "    return z, m, tlp, tlp_grad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FnT1Mzn9T4Hy"
      },
      "source": [
        "The leapfrog_step function is a core part of the Hamiltonian Monte Carlo (HMC) algorithm. It simulates the movement of a particle in a potential energy field (defined by your target_log_prob_and_grad).\n",
        "\n",
        "Here's a breakdown of what the code does:\n",
        "\n",
        "def leapfrog_step(target_log_prob_and_grad, step_size, i, leapfrog_state):: This defines the function leapfrog_step which takes the following arguments:\n",
        "target_log_prob_and_grad: A function that returns both the log probability and its gradient with respect to the parameters (z).\n",
        "step_size: The size of each step in the simulation.\n",
        "i: The current step number (though it's not used in this particular implementation).\n",
        "leapfrog_state: A tuple containing the current state of the simulation: (z, m, tlp, tlp_grad).\n",
        "z: The parameters (position of the particle).\n",
        "m: The momentum of the particle.\n",
        "tlp: The target log probability at the current z.\n",
        "tlp_grad: The gradient of the target log probability at the current z.\n",
        "z, m, tlp, tlp_grad = leapfrog_state: This unpacks the leapfrog_state tuple into its individual components.\n",
        "m += 0.5 * step_size * tlp_grad: This updates the momentum (m) by adding half of the gradient of the log probability multiplied by the step_size. This is the \"half-step\" for momentum at the beginning.\n",
        "z += step_size * m: This updates the parameters (z) by adding the momentum multiplied by the step_size. This is the full step for position.\n",
        "tlp, tlp_grad = target_log_prob_and_grad(z): After updating z, the target log probability (tlp) and its gradient (tlp_grad) are recalculated at the new position.\n",
        "m += 0.5 * step_size * tlp_grad: This is the second \"half-step\" for momentum, using the gradient at the new position z. This symmetric update makes the leapfrog integrator more stable and reversible.\n",
        "return z, m, tlp, tlp_grad: The function returns the updated state after the leapfrog step.\n",
        "In essence, the leapfrog step simulates the physics of a particle (representing your parameters) moving in a landscape where the potential energy is related to the negative log probability of your model. It does this by alternately updating the momentum and position in a way that conserves energy over time, which is crucial for efficient exploration of the parameter space in HMC."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lj2i3ayCTezG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Building a complete HMC step\n",
        "\n",
        "Now we assemble the pieces into a single HMC iteration. The `hmc_step` function orchestrates the full Metropolis-adjusted leapfrog trajectory, and understanding its flow is key to grasping how HMC explores the posterior.\n",
        "\n",
        "**What happens in one HMC step:**\n",
        "\n",
        "1. **Sample fresh momentum.** We draw `m` from a standard normal distribution, independent of the current position `z`. This randomness injects energy into the system and ensures the chain explores different regions of the parameter space.\n",
        "\n",
        "2. **Compute the starting energy.** The Hamiltonian (total energy) splits into kinetic energy $\\frac{1}{2} \\|m\\|^2$ and potential energy $-\\log p(z \\mid x, y)$. Adding these gives us a baseline to compare against after we simulate the trajectory.\n",
        "\n",
        "3. **Simulate Hamiltonian dynamics.** We call `leapfrog_step` repeatedly via `jax.lax.fori_loop`, which unrolls the trajectory without Python-level overhead. Each leapfrog step updates position and momentum in lockstep, preserving volume and keeping the dynamics reversible—properties that help HMC achieve high acceptance rates even in high dimensions.\n",
        "\n",
        "4. **Evaluate the new energy.** After `num_leapfrog_steps` iterations, we recompute kinetic plus potential energy at the proposed state `(new_z, new_m)`. Because the leapfrog integrator introduces small numerical errors, the energy usually drifts slightly.\n",
        "\n",
        "5. **Accept or reject via Metropolis-Hastings.** We compare `energy - new_energy` (the log acceptance ratio). If the new energy is lower, we always accept. If it's higher, we accept with probability $\\exp(\\text{energy} - \\text{new\\_energy})$. This correction ensures our samples remain draws from the true posterior, not just from the approximate Hamiltonian flow.\n",
        "\n",
        "6. **Return diagnostics.** Beyond the updated parameter vector `z`, we also return `is_accepted` (a boolean indicating whether we took the proposal) and `log_accept_ratio` (useful for tuning step size). High acceptance rates suggest we could take bigger steps; very low rates mean the integrator is drifting too far and we should shrink the step size.\n",
        "\n",
        "**Why this matters for accelerators:** By structuring the trajectory loop with `jax.lax.fori_loop` and `partial`, JAX can compile the entire HMC step into a single fused kernel. The CPU or GPU sees a static computation graph with no Python interpreter overhead, which is exactly what Hoffman et al. emphasize for modern hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hmc_step(target_log_prob_and_grad, num_leapfrog_steps, step_size, z, seed):\n",
        "    m_seed, mh_seed = jax.random.split(seed)\n",
        "    tlp, tlp_grad = target_log_prob_and_grad(z)\n",
        "    m = jax.random.normal(m_seed, z.shape)\n",
        "    energy = 0.5 * jnp.square(m).sum() - tlp\n",
        "    new_z, new_m, new_tlp, _ = jax.lax.fori_loop(\n",
        "        0,\n",
        "        num_leapfrog_steps,\n",
        "        partial(leapfrog_step, target_log_prob_and_grad, step_size),\n",
        "        (z, m, tlp, tlp_grad))\n",
        "    new_energy = 0.5 * jnp.square(new_m).sum() - new_tlp\n",
        "    log_accept_ratio = energy - new_energy\n",
        "    is_accepted = jnp.log(jax.random.uniform(mh_seed, [])) < log_accept_ratio\n",
        "    # select the proposed state if accepted\n",
        "    z = jnp.where(is_accepted, new_z, z)\n",
        "    hmc_output = {\"z\": z,\n",
        "                  \"is_accepted\": is_accepted,\n",
        "                  \"log_accept_ratio\": log_accept_ratio}\n",
        "    # hmc_output[\"z\"] has shape [num_dimensions]\n",
        "    return z, hmc_output"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
