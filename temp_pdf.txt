Orbit: Probabilistic Forecast with Exponential Smoothing

Edwin Ng 1 Zhishi Wang 1 Huigang Chen 1 Steve Yang 1 Slawek Smyl 1
{edwinng, zhishiw, huigang, steve.yang, slawek}@uber.com

arXiv:2004.08492v4 [stat.CO] 22 Jan 2021

Abstract
Time series forecasting is an active research topic
in academia as well as industry. Although we
see an increasing amount of adoptions of machine learning methods in solving some of those
forecasting challenges, statistical methods remain
powerful while dealing with low granularity data.
This paper introduces a package Orbit where it
refined Bayesian exponential smoothing model
with the help of probabilistic programming package such as Stan and Pyro. Our model refinements
include additional global trend, transformation for
multiplicative form, noise distribution and choice
of priors. A benchmark study is conducted on a
rich set of time-series data sets for our models
along with other well-known time series models.

1. Introduction
Time series forecasting is one of the most popular and yet
the most challenging tasks, faced by researchers and practitioners. Its industrial applications have a wide range of
areas such as social science, bioinformatics, finance, business operations and revenue forecast. At Uber, time series
forecasting has its applications from demand/supply prediction in the marketplace to the Ads budget optimization in
the marketing data science.
In recent years, machine learning and deep learning methods
(Gers et al., 1999; Huang et al., 2015; Selvin et al., 2017)
have gained increasing attention in the time series forecasting, due to their great ability in capturing the non-linear
trend and complex interactions within multivariate time series. However, the theories for deep learning are still in
the active research and development progress and not well
established yet in the forecasting literature, especially in the
case of univariate time series. At the same time, the blackbox nature of machine learning/deep learning models causes
difficulties in interpretability and explainability (Gunning,
2017; Gilpin et al., 2018). In a benchmark study with differ1

Uber Technologies, Inc..

Preliminary work. To be submitted to StanCon 2020.

ent time series data (Hewamalage et al., 2019), the authors
show mixed performances of various deep learning models
when compared against traditional statistical models.
On the other hand, traditional statistical parametric models
have a well-established theoretical foundation, such as the
popular autoregressive integrated moving average (ARIMA)
(Box & Jenkins, 1968) and exponential smoothing (Gardner Jr, 1985). Recently, researchers from Facebook developed Prophet (Taylor & Letham, 2018), which is based
on an additive model where non-linear trends are fit with
seasonality and holidays.
In this paper we propose a family of refined Bayesian exponential smoothing models, with great flexibility on the
choice of priors, model type specifications, as well as noise
distribution. Our model introduces a novel global trend
term, which works well for short term time series. Most
importantly, our models come with a well crafted compute
software/package in Python, called Orbit (Object-oriented
Bayesian Time Series). Our package leverages the probabilistic programming languages, Stan (Carpenter et al.,
2017) and Pryo (Bingham et al., 2019), for the underlying
MCMC sampling process and optimization. Pyro, developed by researchers at Uber, is a universal probabilistic programming language (PPL) written in Python and supported
by PyTorch and JAX on the backend. Orbit currently has
a subset of the available prediction and sampling methods
available for estimation using Pyro.
The remainder of this paper is organized as follows. In
section 2, a review is given on some popular statistical
parametric time series models. In section 3, we give our
refined model equations, and Orbit package design review.
In section 4, an extensive benchmark study is conducted to
evaluate the proposed models‚Äô performance and compare
with other time series models discussed in the paper. section 5 is about the conclusion and future work. We focus on
the univariate time series in this work.

Orbit: Probabilistic Forecast with Exponential Smoothing

2. Review
Let {y1 , ¬∑ ¬∑ ¬∑ , yt } be a sequence of observations at time t.
Then a point forecast at time T denotes the process of estimating the set of future values of {yÃÇT +1 , ¬∑ ¬∑ ¬∑ , yÃÇT +h } given
information at t ‚àÄt ‚â§ T where h denotes the forecasting
horizon.

The ARMA(p,q) model is defined for stationary data. However, many realistic time series in practice exhibit a nonstationary structure, e.g., time series with trend and seasonality. The ARIMA(p,d,q) (or SARIMA) overcomes this
limitation by including an integration parameter of order
d. In principle, ARIMA works by applying d differencing
transformations to the time series until it becomes stationary
before applying ARMA(p,q).

2.2. The State Space Models

2.2.2. E XPONENTIAL S MOOTHING

State space models is a family of models which can be
written in a general form

Another popular approach is done through exponential
smoothing in a reduced form

2.1. Problem Definition

yt = ZtT Œ±t + t

yÃÇt|t‚àí1 = Z T Œ±t‚àí1

Œ±t+1 = Tt Œ±t + Rt Œ∑t

t = yt ‚àí yÃÇt|t‚àí1

One big advantage is that they are modular, in the sense
that independent state components can be combined by
concatenating their observation vectors Zt and arranging
the other model matrices as elements in a block diagonal
matrix. This provides considerable flexibility for modeling
trend, seasonality, regressors, and potentially other state
components that may be necessary in practice.

Œ±t = T Œ±t‚àí1 + k

However, such form is not unique since the same model can
be expressed in multiple ways. In practice, there are various
reduced forms introduced by researchers. We will go over
some of those widely used in the industry.

This was first described by Box and Jekins (Box, 1998).
Note that yÃÇt|t‚àí1 can be computed recursively. Hyndman
et al. 2008 provide a complete review on such form. In the
literature, it introduces an ETS form with notation ‚ÄúA‚Äù and
‚ÄúM‚Äù which stands for additive and multiplicative respectively.
Well known methods such as Holt-Winter‚Äôs model can be
viewed as ETS(A, A, A) or ETS(A, M, A). For notation
such as Ad , the subscript d denotes a damped factor is introduced. Many of those are implemented under the forecast
package written in the statistical programming language R.

2.2.1. ARIMA

2.2.3. BAYESIAN S TRUCTURAL T IME S ERIES (BSTS)

ARMA model is one of the most commonly used methods
to model univariate time series. ARMA(p,q) combines two
components: AR(p), and MA(q). In AR(p) model, the value
of a given time series, yt , can be estimated using a linear
combination of the p past observations, together with an
error term t and a constant term c:

Scott and Varian (Scott & Varian, 2014) provide a bayesian
approach on a reduced form as such

yt = c + Œ£pi=1 œài yt‚àíi + t

Œ¥t+1 = Œ¥t + Œ∑1t

yt = ¬µt + œÑt + Œ≤ T xt t
¬µt+1 = ¬µt + Œ¥t + Œ∑0t

where œài , ‚àÄi ‚àà {1, ¬∑ ¬∑ ¬∑ , p} denote the model parameters,
and p represents the order of the model. Similarly, the
MA(q) model uses past errors as explanatory variables:
yt = ¬µ + Œ£qi=1 Œ∏i t‚àíi + t
where ¬µ denotes the mean of the observations, Œ∏i , ‚àÄi ‚àà
{1, ¬∑ ¬∑ ¬∑ , q} represents the parameters of the models and q
denotes the order of the model. Essentially, the method
MA(q) models the time series according to the random
errors that occurred in the past q lags.

œÑt+1 = ‚àí

S‚àí1
X

œÑt + Œ∑2t

s=1

This model is named as local linear trend model and is available in the bsts package written in R. It allows additional
regression estimation with coefficients Œ≤.
2.3. Prophet

The model ARMA(p,q) can be constructed by combining
the AR(p) model with the MA(q) model, i.e.,

Researchers at Facebook (Taylor & Letham, 2018) use a
generalized additive model (GAM) with three main components: trend, seasonality, and holidays. They can be
combined in the following equation:

yt = c + Œ£pi=1 œài yt‚àíi + Œ£qi=1 Œ∏i t‚àíi + t

yt = gt + st + ht + t

Orbit: Probabilistic Forecast with Exponential Smoothing

where gt is the piecewise linear or logistic growth curve
to model the non-periodic changes in the time series, st
is the seasonality term, ht is the holiday effect with irregular schedules, and t is the error term. On a high level,
Prophet is framing the forecasting problem as a curve-fitting
exercise rather than looking explicitly at the time based
dependence of each observation within a time series. As
a computational tool/software, moreover, Prophet allows
users to manually supply change points in fitting the trend
term and set the boundaries for saturation growth, which
gives great flexibility in business applications.

3. The Orbit Package

the computation cost by vectorizing the noise generation
process.
One limitation in this model is that it assumes lt > 0, ‚àÄt. To
ensure such condition is satisfied during the training period,
it imposes a requirement such that yt > 0, ‚àÄt.
3.2. Refined Model II - DLT
For use cases with yt ‚àà R, we provide an alternative Damped Local Trend (DLT) model.
Such model is an extension of ETS(A, Ad , A) in Hyndman
et al. 2008, where Œ∏ is known as the damped factor. In the
forecast process, we have

3.1. Refined Model I - LGT
Our proposed Local and Global Trend (LGT) model is an
additive version based on the multiplicative model proposed
in Rlgt (Smyl et al., 2019). In the forecast process, we have

yt = ¬µt + st + rt + t
¬µt = D(t) + lt‚àí1 + Œ∏bt‚àí1
with the update process as such

yt = ¬µt + st + t
Œª
¬µt = lt‚àí1 + Œæ1 bt‚àí1 + Œæ2 lt‚àí1

t ‚àº Student(ŒΩ, 0, œÉ)
œÉ ‚àº HalfCauchy(0, Œ≥0 )
Œª
where lt‚àí1 , Œæ1 bt‚àí1 , Œæ2 lt‚àí1
, st and t can be viewed as level,
local trend, global trend, seasonality and error term, respectively.

In the update process, it is similar to the triple exponential
smoothing form
lt = œÅl (yt ‚àí st ) + (1 ‚àí œÅl )lt‚àí1
bt = œÅb (lt ‚àí lt‚àí1 ) + (1 ‚àí œÅb )bt‚àí1
st+m = œÅs (yt ‚àí lt ) + (1 ‚àí œÅs )st
where œÅl , œÅb and œÅs are the smoothing parameters. Finally,
we fit œÅl , œÅb , œÅs , Œ∏, Œæ, Œª, ŒΩ in our training process while Œ≥0
is a data-driven scaler.
This model structure is similar to the ETS family established
by Rob Hynd (De Livera et al., 2011; Hyndman & Athanasopoulos, 2018) except that the form we proposed is a hybrid
model generalizing both ETS(A, A, A) (when Œæ = 0) and
autoregressive model (when Œæ > 0). A similar approach is
found earlier in (Smyl, Zhang et al., 2015). The advantages
over Rlgt with such form are two-fold. First, the computation is more effective on the additive form, where we can
apply simple log transformation to maintain multiplicative
properties. Second, œÉ in Rlgt is parameterized with yt which
introduces dependency in noise generation, while œÉ is refined as an independent noise in LGT. This change reduces

gt = D(t)
lt = œÅl (yt ‚àí gt ‚àí st ‚àí rt ) + (1 ‚àí œÅl )(lt‚àí1 + bt‚àí1 )
bt = œÅb (lt ‚àí lt‚àí1 ) + (1 ‚àí œÅb )Œ∏bt‚àí1
st+m = œÅs (yt ‚àí lt ‚àí rt ) + (1 ‚àí œÅs )st
rt = Œ£j Œ≤j xjt
There are a few choices of D(t) as a deterministic global
trend. Options such as linear, log-linear and logistic are
included in the package. Another feature of DLT is the
introduction of regression component rt . This serves the
purpose of nowcasting or forecasting when exogenous regressors are known such as events and holidays. Without
loss of generality, assume
Œ≤j ‚àº Normal(¬µj , œÉj )
where ¬µj = 0 and œÉj = 1 by default as a non-informative
prior. There are more choices of priors for the regression
component in the package.
3.3. Multiplicative Form
In previous section, we derive a general additive form for
LGT and DLT. It is trivial that by using transformation
yt0 = ln(yt ), our forecast process yields a multiplicative
form as such
yÃÇt = ¬µ0t ¬∑ s0t ¬∑ rt0
In the later benchmarking process, we used models fitted in
such multiplicative form to report performance metrics.

Orbit: Probabilistic Forecast with Exponential Smoothing

3.4. Package Design

4. Model Benchmark

Orbit is our Python package to implement the refined models
discussed above. This package is written and designed from
a strict object oriented perspective with the goals of reusability, ease of maintenance, and high efficiency.

4.1. Data

The base Estimator class contains generic logic to handle
interaction with the underlying inference engine (e.g PyStan,
Pyro) along with utilities to load and save Orbit models, and
the specifics of the model are implemented in each model
class. Figure 1 is the overall workflow of Orbit package.

We performed a comprehensive benchmark study on five
datasets:
‚Ä¢ US and Canada rider first-trips with Uber (20 weekly
series by city)
‚Ä¢ US and Canada driver weekly first-trips with Uber (20
weekly series by city)
‚Ä¢ Worldwide first-orders with Uber Eats(15 daily series
by country)
‚Ä¢ M3 series (1428 monthly series)
‚Ä¢ M4 series (359 weekly series)
where M3/M4 time series are well-known in the forecast
community (Makridakis et al., 2018).
4.2. Performance Metric
We use symmetric mean absolute percentage error
(SMAPE), a widely adopted forecast metric, as our performance benchmark metric

SMAPE =

h
X

|Ft ‚àí At |
(|F
t | + |At |)/2
t=1

where Xt represents the value measured at time t and h is
the forecast horizon.
Here h is the forecast horizon which can also be considered
as the ‚Äúholdouts‚Äù in a backtest process. Following what
competitions suggested, we use 13 forecast horizon and
18 forecast horizon, respectively, for M4 weekly and M3
monthly series with 1 split; for Uber datasets, we use h =
13, 3 splits with 26 incremental steps for weekly series and
h = 28, 4 splits and 14 incremental steps for daily series.
With multiple splits, we expect more robust result. The
calculation is done with the help of our backtest utilities
built in the Orbit package.
4.3. Results

Figure 1. Overall Design of Orbit Package.

We compared our proposed models, LGT and DLT, to other
popular time series models such as SARIMA (Seabold &
Perktold, 2010) and Facebook Prophet (Taylor & Letham,
2018). Both Prophet and Orbit models use Maximum A
Posterior (MAP) estimates and they are configured as similar
as possible in terms of optimization and seasonality settings.
For SARIMA, we fit the (1, 1, 1) √ó (1, 0, 0, )S structure by
maximum likelihood estimation (MLE) where S represents
the choice of seasonality.

Orbit: Probabilistic Forecast with Exponential Smoothing

Li, P., and Riddell, A. Stan: A probabilistic programming
language. Journal of statistical software, 76(1), 2017.

Table 1. Model Average SMAPE Comparison
M ODEL
R IDER - WEEKLY
D RIVER - WEEKLY
E ATER - DAILY
M4- WEEKLY
M3- MONTHLY

LGT
0.108 (0.030)
0.205 (0.098)
0.178 (0.036)
0.077 (0.073)
0.145 (0.155)

DLT
0.106 (0.033)
0.206 (0.095)
0.182 (0.043)
0.081 (0.080)
0.148 (0.158)

P ROPHET
0.121 (0.035)
0.274 (0.162)
0.309 (0.055)
0.192 (0.321)
0.193 (0.234)

SARIMA
0.110 (0.041)
0.207 (0.123)
0.221 (0.040)
0.076 (0.131)
0.150 (0.171)

De Livera, A. M., Hyndman, R. J., and Snyder, R. D. Forecasting time series with complex seasonal patterns using
exponential smoothing. Journal of the American statistical association, 106(496):1513‚Äì1527, 2011.

Within a dataset, the models in consideration were run on
each time series separately, and then we report the aggregated metrics. Table 1 gives the average and the standard
deviation (within parentheses) of SMAPE across different
models and datasets.

Gardner Jr, E. S. Exponential smoothing: The state of the
art. Journal of forecasting, 4(1):1‚Äì28, 1985.

It shows that our models consistently deliver better accuracy
than other candidate time series models in terms of SMAPE.
Orbit is also computationally efficient. For example, the
average compute time per series with full MCMC sampling
and prediction from a subset of M4 weekly data is about
2.5 minutes and 16 ms. The run time for the same series
in Prophet is about 10 minutes for sampling and 2.4 s for
prediction. That‚Äôs a 4x speed up in training, and orders of
magnitude difference in prediction.

Gilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M.,
and Kagal, L. Explaining explanations: An overview of
interpretability of machine learning. In 2018 IEEE 5th
International Conference on data science and advanced
analytics (DSAA), pp. 80‚Äì89. IEEE, 2018.

Code and M3/M4 data used in this benchmark study are
available upon request.

Hewamalage, H., Bergmeir, C., and Bandara, K. Recurrent
neural networks for time series forecasting: Current status
and future directions. arXiv preprint arXiv:1909.00590,
2019.

5. Conclusion
We have shown that our proposed models outperform the
baseline time series models consistently in terms of SMAPE
metrics. Furthermore, we also identified compute cost improvements when using the Orbit package. For our future
work, we will continue to actively maintain the package, incorporate new models, and provide new features or enhancements (to support dual seasonality, fully Pyro integration,
etc).

References
Bingham, E., Chen, J. P., Jankowiak, M., Obermeyer, F.,
Pradhan, N., Karaletsos, T., Singh, R., Szerlip, P., Horsfall, P., and Goodman, N. D. Pyro: Deep universal probabilistic programming. The Journal of Machine Learning
Research, 20(1):973‚Äì978, 2019.
Box, G. P, Jenkins, GM and Reinsel, GC. Time series
analysis: forecasting and control, volume 54, pp. 176‚Äì
180. 1998.
Box, G. E. and Jenkins, G. M. Some recent advances in
forecasting and control. Journal of the Royal Statistical
Society. Series C (Applied Statistics), 17(2):91‚Äì109, 1968.
Carpenter, B., Gelman, A., Hoffman, M. D., Lee, D.,
Goodrich, B., Betancourt, M., Brubaker, M., Guo, J.,

Gers, F. A., Schmidhuber, J., and Cummins, F. Learning to
forget: Continual prediction with lstm. 1999.

Gunning, D. Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd
Web, 2, 2017.

Huang, Z., Xu, W., and Yu, K. Bidirectional lstm-crf models
for sequence tagging. arXiv preprint arXiv:1508.01991,
2015.
Hyndman, R., Koehler, A. B., Ord, J. K., and Snyder, R. D.
Forecasting with exponential smoothing: the state space
approach. Springer Science & Business Media, 2008.
Hyndman, R. J. and Athanasopoulos, G. Forecasting: principles and practice. OTexts, 2018.
Hyndman, R. J., Athanasopoulos, G., Bergmeir, C., Caceres, G., Chhay, L., O‚ÄôHara-Wild, M., Petropoulos, F.,
Razbash, S., Wang, E., and Yasmeen, F. forecast: Forecasting functions for time series and linear models. 2018.
Makridakis, S., Spiliotis, E., and Assimakopoulos, V. The
m4 competition: Results, findings, conclusion and way
forward. International Journal of Forecasting, 34(4):
802‚Äì808, 2018.
Scott, S. L. and Varian, H. R. Predicting the present with
bayesian structural time series. International Journal of
Mathematical Modelling and Numerical Optimisation, 5
(1-2):4‚Äì23, 2014.
Seabold, S. and Perktold, J. statsmodels: Econometric and
statistical modeling with python. In 9th Python in Science
Conference, 2010. Package version 0.11.1.

Orbit: Probabilistic Forecast with Exponential Smoothing

Selvin, S., Vinayakumar, R., Gopalakrishnan, E., Menon,
V. K., and Soman, K. Stock price prediction using lstm,
rnn and cnn-sliding window model. In 2017 international
conference on advances in computing, communications
and informatics (icacci), pp. 1643‚Äì1647. IEEE, 2017.
Smyl, S., Bergmeir, C., Wibowo, E., and Ng, T. W. Bayesian
exponential smoothing models with trend modifications.
https://cran.r-project.org/web/packages/Rlgt/index.html,
2019.
Taylor, S. J. and Letham, B. Forecasting at scale. The American Statistician, 72(1):37‚Äì45, 2018. Package version
0.7.1.

